{"0": {
    "doc": "Advanced Techniques",
    "title": "Advanced Techniques",
    "content": "This section of the guide covers more advance techniques that you can utilize; with the section on GIMP acting as a quick guide for image manipulation. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/advanced_techniques.html",
    "relUrl": "/docs/advanced_techniques/advanced_techniques.html"
  },"1": {
    "doc": "Automation",
    "title": "Automating Chunky - Scripting basics",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/automation.html#automating-chunky---scripting-basics",
    "relUrl": "/docs/advanced_techniques/automation.html#automating-chunky---scripting-basics"
  },"2": {
    "doc": "Automation",
    "title": "Table of contents",
    "content": ". | Batch Rendering . | .batch Rendering (Windows) | Python3 batch rendering script | . | Batch “manual” denoising . | .batch “manual” denoising | Drag’n’drop .batch denoising | Python3 based batch denoising? | . | . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/automation.html#table-of-contents",
    "relUrl": "/docs/advanced_techniques/automation.html#table-of-contents"
  },"3": {
    "doc": "Automation",
    "title": "Batch Rendering",
    "content": "This section of the guide covers scripts which can be used to automate the Rendering of multiple scenes without any user input.batch Rendering (Windows) . This batch script looks for a txt file called queue.txt located in the same directory as this .bat script &amp; ChunkyLauncher.jar. @ECHO off SET queue=queue.txt SET /A sppTarget=1024 SET /A dumpFrequency=256 SET outputMode=PFM FOR /F %%a IN (%queue%) DO ( ECHO Rendering %%a java -jar ChunkyLauncher.jar -set sppTarget %sppTarget% %%a java -jar ChunkyLauncher.jar -set dumpFrequency %dumpFrequency% %%s java -jar ChunkyLauncher.jar -set outputMode %outputMode% %%a java -jar ChunkyLauncher.jar -render %%a -threads 8 ) PAUSE . Python3 batch rendering script . Same concept as before. Just need a queue.txt file to list all the scenes and then you need to run the Python script. \"\"\" AutoChunky v1.0.2 Written by colebob9 Coded in Python 3 Released under the MIT license Source code repo: https://github.com/colebob9/AutoChunky \"\"\" import shlex import subprocess import time # Config chunkyPath = \"ChunkyLauncher.jar\" threads = 4 # End config # Title print(\"AutoChunky v1.0.2\") print(\"Written by colebob9\") print(\"Source Code on GitHub.com/colebob9/AutoChunky\") # Checking queue file print(\"Reading queue file...\") print('') f = open(\"queue.txt\") queueList = f.readlines() queueList = [s.rstrip() for s in queueList] # stripping off \\n f.close() print(\"Currently queued scenes:\") print(queueList) print('') # Queue and render command for r in queueList: print('') print(\"Now rendering: \" + r) print('') subprocess.call(shlex.split(\"java -jar %s -render %s -threads %s\" % (chunkyPath, r, threads))) print('waiting for 2') time.sleep(2) exit . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/automation.html#batch-rendering",
    "relUrl": "/docs/advanced_techniques/automation.html#batch-rendering"
  },"4": {
    "doc": "Automation",
    "title": "Batch “manual” denoising",
    "content": "Used a batch rendering script or otherwise and have a load of noisy images that need denoising? This is the place for you! .batch “manual” denoising . This is a very old script of mine which I used around the time AI base denoisers first came out mainly to denoise the outputs of my interpolation scripts (at 16 SPP they are very noisy). This .bat script needs to be located in the same directory as the images to denoise. SET FILE_EXTENSION=png SET PATH_TO_DENOISER=C:\\Denoiser_v2.1 SET OUTPUT_PREFIX=denoised_ for /r %%v in (*.%FILE_EXTENSION%) do %PATH_TO_DENOISER%\\Denoiser.exe -i \"%%~nv.%FILE_EXTENSION%\" -o \"%OUTPUT_PREFIX%%%~nv.%FILE_EXTENSION%\" cmd /k . Drag’n’drop .batch denoising . Once setup with the correct paths to the denoiser, drag and drop multiple noisy .png or .pfm files onto this .bat script for it to auto detect related .albedo and .normal feature images and denoise them… hopefully. GitHub . Python3 based batch denoising? . Nothing yet. Maybe someone should make something and it’ll be linked here. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/automation.html#batch-manual-denoising",
    "relUrl": "/docs/advanced_techniques/automation.html#batch-manual-denoising"
  },"5": {
    "doc": "Automation",
    "title": "Automation",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/automation.html",
    "relUrl": "/docs/advanced_techniques/automation.html"
  },"6": {
    "doc": "Cubic Chunks",
    "title": "Cubic Chunks",
    "content": ". This page is no longer required at this time. Chunky 2.4.0 has support for Cubic Chunks for Minecraft 1.10 to 1.12.2. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/cubicchunks.html#cubic-chunks",
    "relUrl": "/docs/advanced_techniques/cubicchunks.html#cubic-chunks"
  },"7": {
    "doc": "Cubic Chunks",
    "title": "Cubic Chunks",
    "content": ". ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/cubicchunks.html",
    "relUrl": "/docs/advanced_techniques/cubicchunks.html"
  },"8": {
    "doc": "Denoising",
    "title": "Denoising",
    "content": "todo - need to scale downsampling to 540p . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/denoising.html",
    "relUrl": "/docs/advanced_techniques/denoising.html"
  },"9": {
    "doc": "Denoising",
    "title": "Table of contents",
    "content": ". | Denoising . | Super Sample Downscaling Denoising . | Original examples | . | Split Pass Denoising . | llbit’s example | . | Multi-plane . | Example | . | AI Based Denoising &amp; Plugins . | Example | Example of painted effect | leMaik’s Denoising Plugin | Albedo Map | Normal Map | . | Split Pass + AI Denoising . | Example 1 | Example 2 | A simple proof of concept with Layer mode: Screen | . | . | . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/denoising.html#table-of-contents",
    "relUrl": "/docs/advanced_techniques/denoising.html#table-of-contents"
  },"10": {
    "doc": "Denoising",
    "title": "Super Sample Downscaling Denoising",
    "content": "A technique first covered back in October 2014. The basic premise is to render a scene at a higher than target resolution, apply a Gaussian blur to the whole image, and then scale it down to the target resolution. In any case it is assumed you render the scene for the same duration. Generally this means that a scene at say 1920x1080 with a target of 1024 SPP could be rendered at 3840x2160 at 256 SPP- These should take the same amount of time to render and the 2160p image would have perceptually less noise than the same scene rendered natively at 1080p. The noise is still present but given the higher resolution each pixel of noise takes up less screen space compared to the “noiseless” data from the sun/sky. Blurring and scaling to target resolution should result in better results than rendering at native resolution. Increasing the canvas resolution does increase memory consumption. Original examples . 540p Native . 1080p (2x supersampling) . 2160p (4x supersampling) . 4320p (8x supersampling) . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/denoising.html#super-sample-downscaling-denoising",
    "relUrl": "/docs/advanced_techniques/denoising.html#super-sample-downscaling-denoising"
  },"11": {
    "doc": "Denoising",
    "title": "Split Pass Denoising",
    "content": "An approach llbit showcased in September 2015. By rendering the scene twice, first with a “Sunlight Pass” to 200 SPP and then a “Raw Emitter Pass” to 400 SPP. The noisy emitter pass could then be filtered with a Selective Gaussian Blur in GIMP and then combined. Comparing a typical 400 SPP Sunlight + Raw Emitter image to the Sunlight + Filtered Emitter image it was seen that the lighting was softer and there was less noticeable noise. The different lighting passes can be setup through the Lighting and the Sky &amp; Fog tabs. For a Sunlight pass you would simply need to disable emitters. For an Emitter pass things are a bit more complex: Enable emitters (if not done), disable sunlight, and Sky mode: black OR enable transparent sky. llbit’s example . Sunlight pass at 200 SPP . Emitter pass at 400 SPP . Filtered emitter pass . Sunlight + filtered emitter pass . Same scene rendered at 400 SPP . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/denoising.html#split-pass-denoising",
    "relUrl": "/docs/advanced_techniques/denoising.html#split-pass-denoising"
  },"12": {
    "doc": "Denoising",
    "title": "Multi-plane",
    "content": "A technique which I never released; Covers rendering a scene with a clear and distinct fore and background elements. By rendering the complete scene with emitters at 0.01, emitters would light up but not emit visible light. This would lead to zero emitter noise in the distance. Meaning lower SPPs could be used for this element. Foreground element should be loaded with a reduced number of chunks, to speed up rendering speed, with the chosen emittance. With a careful selection of Chunks and combination of the two elements in post a reasonable result can be achieved. Example . Foreground selection of 76 chunks . 1024 SPP @ 932k SPS - aka it’s fast . Background selection of 1877 chunks . 128 SPP @ 482k SPS - aka it’s slow . Crude composite . The key issue with this technique is that the lighting information in the foreground element would not completely match the background. Further mitigations would be to instead combine this Multi-plane technique with Split Pass to further speed up rendering and retain bounce lighting from Sun/Sky present within the background pass. Using Split Pass would also mean that less effort is required to recombine the completed results. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/denoising.html#multi-plane",
    "relUrl": "/docs/advanced_techniques/denoising.html#multi-plane"
  },"13": {
    "doc": "Denoising",
    "title": "AI Based Denoising &amp; Plugins",
    "content": "First showcased by u/StaysAwakeAllWeek in Nov 2018, this was the first time an AI based denoiser was mentioned on r/Chunky. Unfortunately at the time it was limited to just Nvidia GPUs. A few months later I discovered Intel’s Open Image Denoise; an AI based denoiser that works on any CPU with SSE4.1 support. Example . | SPP | RAW | OIDN | . | 512 | | | . | 2048 | | | . | 4096 | | | . | 8192 | | | . As some of you may have noticed while AI based denoisers work wonders there are a few issues with the outputted images. Noteworthy visual artifacts are the deformed blocks, blurred textures, and the painted effect you can often see. Below you can see an extreme case where a 32 SPP scene lit mostly by emitters was denoised. Example of painted effect . leMaik’s Denoising Plugin . Some of these issues can be resolved or mitigated by using leMaik’s Denoising Plugin which has the ability to, not only to automatically denoise a scene once the target SPP is reached but, render auxiliary feature images / AOVs (Arbitrary Output Variables) to provide additional information to the denoiser. Unfortunately you still can’t expect the AI denoiser to work real magic; If there is too much noise you will still get a painted effect. Albedo Map . The Albedo map is a feature image that provides the largest quality bump to the denoiser. It’s basically just a representation of the texture information within the scene independant of shading (lighting) or viewing angle. This map tends to help restore texture details. Normal Map . The Normal map is another feature image that can help. In order to use a Normal map you need to provide the Denoiser with the Albedo map first. This map tends to help restore block shape. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/denoising.html#ai-based-denoising--plugins",
    "relUrl": "/docs/advanced_techniques/denoising.html#ai-based-denoising--plugins"
  },"14": {
    "doc": "Denoising",
    "title": "Split Pass + AI Denoising",
    "content": "For some scenes where we have access to both Sun/Sky light and Emitters. This technique is… well it is mostly a combinaton of llbit’s Split Pass and AI denoising with a touch of my Multi-plane. The goal of this method is to min-max speed and quality as much as possible. As many would probably know by now; Scenes which receive most of their lighting directly from the Sun or Sky generally do not require SPPs past 1000 SPP, and often work fine with 64-256 SPP, unlike scenes with emitters that can require 8000 SPP or more. Subjecting the whole scene to an AI based denoiser when it is not necessary is what this combination technique aims to solve. We take llbit’s Split Pass Denoising and instead of feeding the emitter pass through a simple Gaussian blur we use an AI based denoiser like Nvidia Optix or OIDN. Given the Sun, Sky, and Fog pass is not put through the denoiser any fine details or grain from this pass will be kept. This is most noticeable when looking at the fog as this tends to be heavily blurred by AI Denoisers. Any part of the scene that does not receive lighting from the Sun, Sky, and Fog pass but instead is lit from the emitter pass will still unfortunately be subjected the the pitfalls of the AI Denoiser. However, given that we can reduce the number of chunks loaded for this pass, and that we do not need to deal with fog, it should be possible to achieve a high SPP relatively quickly. Put simply: Render the Sun, Sky, and Fog in one pass. In another pass render the emitters and denoise them with an AI base denoiser like Open Image Denoise or Nvidia Optix. These two passes can be combined using GIMP by setting the layer mode to Screen. Example 1 . Sun + Sky + Fog Pass at 256 SPP . Raw Emitter Pass at 2048 SPP . AI denoised Emitter Pass . Combined SSF + E_dn using GIMP and layer mode set to screen . Example 2 . Sun + Sky + Fog Pass at 256 SPP . Raw Emitter Pass at 2048 SPP . AI denoised Emitter Pass . Combined SSF + E_dn using GIMP and layer mode set to screen . A simple proof of concept with Layer mode: Screen . Below is a showcase of further subdividing light sources into different passes and the combined composite VS a typical render. This is merely to showcase that splitting render passes and combining in post can produce equivalent results to a more typical render. Of course subdividing a scene into this many passes and rendering to such a high SPP is not required for Split Pass + AI Denoising. | Sun pass | Sky pass | Emitter pass | . | | | | . | SSE_Comp | | Typical bake | . | | | . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/denoising.html#split-pass--ai-denoising",
    "relUrl": "/docs/advanced_techniques/denoising.html#split-pass--ai-denoising"
  },"15": {
    "doc": "GIMP",
    "title": "GNU Image Manipulation Program - A quick rundown",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/gimp.html#gnu-image-manipulation-program---a-quick-rundown",
    "relUrl": "/docs/advanced_techniques/gimp.html#gnu-image-manipulation-program---a-quick-rundown"
  },"16": {
    "doc": "GIMP",
    "title": "Table of contents",
    "content": ". | Other | Filtering . | Blur . | Gaussian blur | Selective Gaussian blur | . | . | . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/gimp.html#table-of-contents",
    "relUrl": "/docs/advanced_techniques/gimp.html#table-of-contents"
  },"17": {
    "doc": "GIMP",
    "title": "Other",
    "content": ". ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/gimp.html#other",
    "relUrl": "/docs/advanced_techniques/gimp.html#other"
  },"18": {
    "doc": "GIMP",
    "title": "Filtering",
    "content": "With the chosen layer selected over in the Layers window. You can apply various Filters to it by accessing the menu from the Menu Bar. Blur . Gaussian blur . Performs an averaging of neighboring pixels with the normal distribution as weighting . This is the most basic blur filter available and really quick. Has the option to change blur size in X and Y independently. . Selective Gaussian blur . Blur neighboring pixels, but only in low contrast areas . This filter is more advanced and lets you set a threshold so that only similar pixels are blurred together. Should help retain sharp edges while reducing grain. This filter is much slower than Gaussian blur but this probably won’t amount to much addtional processing time unless working with a high resolution image. Unfortunately you cannot control blur in X&amp;Y independently instead it is controlled by a Blur radius. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/gimp.html#filtering",
    "relUrl": "/docs/advanced_techniques/gimp.html#filtering"
  },"19": {
    "doc": "GIMP",
    "title": "GIMP",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/gimp.html",
    "relUrl": "/docs/advanced_techniques/gimp.html"
  },"20": {
    "doc": "Help Wanted",
    "title": "Help Wanted",
    "content": "This page is no longer required. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/helpwanted/helpwanted.html",
    "relUrl": "/docs/helpwanted/helpwanted.html"
  },"21": {
    "doc": "Home",
    "title": "jackjt8's Guide to Chunky ",
    "content": "Chunky is a Minecraft rendering tool that uses Path Tracing to create realistic images of your Minecraft worlds. This guide was orginally created back when we did not have access to make changes to Chunky's original site at chunky.llbit.se. As Chunky has developed over the years things have changed. We now have a new and up to date site, Chunky Manual , which we have worked tirelessly to create. As such the majority of the content here is no longer required. This guide will live on as a place to find more advanced techniques. Witchcraft and Wizardry - The Floo Network . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/",
    "relUrl": "/"
  },"22": {
    "doc": "Home",
    "title": "jackjt8's (unscripted) Video Guide to Chunky",
    "content": ". ",
    "url": "https://jackjt8.github.io/ChunkyGuide/",
    "relUrl": "/"
  },"23": {
    "doc": "Home",
    "title": "About Me",
    "content": "I have used Chunky for over 7 years now and in this time I have produced in excess of 400 renders. Over the years I have learnt so much and I feel it's time to put everything in writing. This guide is an accumulation of knowledge and I very much hope that others will add to it so it can evolve and become something more. Just in case anyone wants to see my work, you can find all of it in this Google Drive folder. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/",
    "relUrl": "/"
  },"24": {
    "doc": "Home",
    "title": "Home",
    "content": ". ",
    "url": "https://jackjt8.github.io/ChunkyGuide/",
    "relUrl": "/"
  },"25": {
    "doc": "Introduction",
    "title": "Introduction to Chunky",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/introduction/introduction.html#introduction-to-chunky",
    "relUrl": "/docs/introduction/introduction.html#introduction-to-chunky"
  },"26": {
    "doc": "Introduction",
    "title": "Table of contents",
    "content": ". | Path Tracing | Emitter Sampling Strategy (ESS) | . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/introduction/introduction.html#table-of-contents",
    "relUrl": "/docs/introduction/introduction.html#table-of-contents"
  },"27": {
    "doc": "Introduction",
    "title": "Path Tracing",
    "content": "Probably best you just read llbit’s page on this one. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/introduction/introduction.html#path-tracing",
    "relUrl": "/docs/introduction/introduction.html#path-tracing"
  },"28": {
    "doc": "Introduction",
    "title": "Emitter Sampling Strategy (ESS)",
    "content": "With every intersection the sun is sampled adding its contribution to the ray without the need for random sampling; which is one of the main reasons why the convergence of scenes lit by the sun (and sky) occurs quickly. Emitters on the other hand need to be directly hit for them to contribute which has a lower probability of occuring; espically with smaller emitters like torches. ESS enables similar sampling to which the sun uses and, in theory, should lead to faster convergence. However, whereas there is only a single sun present in the scene, there can be multiple emitters and those at distances where they will not contribute much to the image- For this we have the emittergrid which holds the positions of emitters within cells. When sampling we would only consider emitters within the cell at the intersection and adjacent cells; That way the cost of processing the addtional samples is minimalised. With ESS ONE only a single emitter is sampled per intersection and cell(+adj) &amp; for ALL every emitter in the cell(+adj) is sampled per intersection. Sampling emitters increases the rendering cost but reduces the required samples. ESS:ONE tends to be very similar to ESS:NONE with ESS:ALL being the “slowest” but potentially fastest to converge- ALL also tends to result in much brighter images than NONE/ONE so a reduction in exposure or emittance value is required to compensate for this. The emitter grid (cell) size changes the size of the cells which can impact how many emitters would need to be sampled per intersection, potentially improving performance, however this could lead to issues such as light cut-off if set too low. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/introduction/introduction.html#emitter-sampling-strategy-ess",
    "relUrl": "/docs/introduction/introduction.html#emitter-sampling-strategy-ess"
  },"29": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/introduction/introduction.html",
    "relUrl": "/docs/introduction/introduction.html"
  },"30": {
    "doc": "Lense Shift",
    "title": "Lense Shift",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/lenseshift.html",
    "relUrl": "/docs/advanced_techniques/lenseshift.html"
  },"31": {
    "doc": "Lense Shift",
    "title": "Table of contents",
    "content": ". | Canvas subdivision for non parallel projections . | Subdivision by half | Subdivision by quarters | Ninth part subdivision | Sixteenth part subdivision | . | . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/lenseshift.html#table-of-contents",
    "relUrl": "/docs/advanced_techniques/lenseshift.html#table-of-contents"
  },"32": {
    "doc": "Lense Shift",
    "title": "Canvas subdivision for non parallel projections",
    "content": "Take a canvas with the following properties: WxH with a fov defined as F . Example: 960x540 (16/9) @ 70 FoV . Subdivision by half . To calculate the required lense shift values to render two smaller canvases (side-by-side) with a [W/2 x H] = [w x h] size, given it’s relation to canvas height, only the ratio between the w &amp; h of the sub-canvas is required to be calulated . x = +/- 1/2 * w/h . No FoV change required. This gives us the following two lense shift combinations: . Examples at 480x540 @ 70 FoV . | [-0.5w/h,0] | [0.5w/h,0] | . | | | . E.G. For the provided subdivisions the offsets would be [+-0.444444444,0]. Subdivision by quarters . To calculate the required lense shift values to render four smaller canvases with a [W/2 x H/2] = [w x h] size with a F/2 = f FoV, given it’s relation to canvas height, only the ratio between the w &amp; h of the sub-canvas is required to be calulated: . x = +/- 1/2 * w/h . &amp; . y = +/- 1/2 . This gives us the following lense shift combinations: . Examples at 480x270 @ 35 FoV . | [-0.5w/h,+1/2] | [0.5w/h,+1/2] | . | | | . | [-0.5w/h,-1/2] | [0.5w/h,-1/2] | . | | . E.G. For the provided subdivisions the offsets would be [+-0.888888889, +-0.5]. Ninth part subdivision . To calculate the required lense shift values to render nine smaller canvases with a [W/3 x H/3] = [w x h] size with a F/3? = f FoV, given it’s relation to canvas height, only the ratio between the w &amp; h of the sub-canvas is required to be calulated: . WIP . Sixteenth part subdivision . To calculate the required lense shift values to render sixteen smaller canvases with a [W/4 x H/4] = [w x h] size with a F/4? = f FoV, given it’s relation to canvas height, only the ratio between the w &amp; h of the sub-canvas is required to be calulated: . WIP . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/lenseshift.html#canvas-subdivision-for-non-parallel-projections",
    "relUrl": "/docs/advanced_techniques/lenseshift.html#canvas-subdivision-for-non-parallel-projections"
  },"33": {
    "doc": "Weather",
    "title": "Weather",
    "content": " ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/weather.html",
    "relUrl": "/docs/advanced_techniques/weather.html"
  },"34": {
    "doc": "Weather",
    "title": "Table of contents",
    "content": ". | Snow . | Snow layers | Adding it all together | . | Rain | . ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/weather.html#table-of-contents",
    "relUrl": "/docs/advanced_techniques/weather.html#table-of-contents"
  },"35": {
    "doc": "Weather",
    "title": "Snow",
    "content": "Credit to MalignantLugnut on discord. The method outlined here is directly taken from comments made by MalignantLugnut on discord. All images shown in this article are taken from comments made by MalignantLugnut. Snow layers . The following layers are made by creating an alpha mask of various objects within the scene at varying distances from the camera. This process could probably be done a lot faster if using a depth map produced from the AOV plugin. The hatched area in the following images is transparent. Background - Most of this layer is covered due to the distance we want it at. Midground - Most of this layer is visible and only things really close to the camera is blocking the layer. . Foreground - Snow in this layer covers everything and is not blocked by anything. It should be noted that the lines visible in the image are due to a botched copy/paste to extend the snow layer. I would recommend use of a titling filter. The lines are not visible in the final render so it was noted at the time that this is likely a non-issue. Adding it all together . Here is the one with the background snow. See how the front building, the middle buildings, the Christmas tree and the reindeer and sleigh are clear? That’s because they are positioned in FRONT of where that snow should show up. Next closest Layer, The tree and center buildings are now behind the snow, but the closest building and some of the candy canes are closer still, so no snow on them. Next Layer, pretty much everything but a portion of the closest building is behind the snow. And the final snow layer just blankets the whole image. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/weather.html#snow",
    "relUrl": "/docs/advanced_techniques/weather.html#snow"
  },"36": {
    "doc": "Weather",
    "title": "Rain",
    "content": "Credit to MalignantLugnut on discord. The method used to produce the following image was not fully detailed but it can be assumed that it is the same as the one used from Snow above just with a layered rain texture. One aspect that is not shown in MalignantLugnut’s work are puddles. One such solution to this is to mix in different blocks to floors in game which then could then have the Specular value changed in Chunky. Blocks with Specular become reflective and with a bit of tuning the result can look pretty good. Below is an example render of mine that showcases puddles and other wet surfaces. I should note that this was rendered back in Chunky 1.4.X and that we now have Metalness and Smoothness which would further aid in the production of wet surfaces. ",
    "url": "https://jackjt8.github.io/ChunkyGuide/docs/advanced_techniques/weather.html#rain",
    "relUrl": "/docs/advanced_techniques/weather.html#rain"
  }
}
